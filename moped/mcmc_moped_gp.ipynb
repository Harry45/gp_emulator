{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import dill \n",
    "\n",
    "# import likelihood script \n",
    "import mcmc_emulator_moped import emu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup For the Emulator (MOPED)\n",
    "<br>\n",
    "<div style=\"text-align: justify\">Here we are emulating each of the MOPED coefficients with a Gaussian Process and we assume that the GPs are already trained and stored. The third line below is just loading all the GPs. If setRandom = True, this will use the uncertainty of the GP inside the likelihood code, if set False, the mean of the GP is used. nRealisation is the number of samples we are drawing when marginalising over the latent function. We also assume that the MOPED vectors and compressed data are already pre-computed and stored.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Integrations performed at resolution of histogram!\n"
     ]
    }
   ],
   "source": [
    "emulator_moped = emu.emulator(file_settings = 'settingsMoped', setRandom = False, nRealisation = 20)\n",
    "emulator_moped.loadGPs('gps/')\n",
    "emulator_moped.priors_moped()\n",
    "emulator_moped.loadMopedVectors(fileName = '3.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MCMC \n",
    "<br>\n",
    "<div style=\"text-align: justify\">With the mean of the GP, this is quick - around 1.5 - 2.5 hours (compared to CLASS which takes around 44 hours). With the GP uncertainty, this depends very much on the number of the training points. For 1000 training point, the speed is comparable to the GP mean. However, computing the GP uncertainty gets worse as the number of training points increases. Note that, we also delete the GPs (because there is a matrix of size $N_{\\textrm{train}}\\times N_{\\textrm{train}}$ stored) before saving the MCMC samples below. The parameters are in the following format:</div>\n",
    "\n",
    "$$\n",
    "\\left[\\Omega_{\\textrm{cdm}}h^{2},\\,\\Omega_{\\textrm{b}}h^{2},\\,\\textrm{ln}\\left(10^{10}A_{\\textrm{s}}\\right),\\,n_{\\textrm{s}},\\,h,\\,A_{\\textrm{bary}},\\,A_{1},\\,A_{2},\\,A_{3},\\,A_{\\textrm{IA}},\\,\\Sigma m_{\\nu},\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params   = np.array([ 0.1295,  0.0224,  2.895 ,  0.9948,  0.7411,  1.0078,  0.0289, 0.0133, -0.0087, -1.9163,  0.5692])\n",
    "filenams = 'mcmc_samples/samples_emulator_moped_1000'\n",
    "\n",
    "# eps     =  np.array([1E-3, 1E-4, 0.01, 0.01, 1E-3, 0.1, 1E-4, 1E-4, 1E-4, 0.1, 0.01])\n",
    "# samples = emulator_moped.emceeSampler_moped(params, eps, nSamples = 15000, nwalkers = 22)\n",
    "# emulator_moped.deleteGPs()\n",
    "# with open(directory+filename, 'wb') as g:\n",
    "#     dill.dump(wholeSampler, g)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
